{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross-Validation - Model Selection\n",
        "## TASK 6: Model Selection - 5-Fold Cross-Validation\n",
        "\n",
        "**Date:** December 30, 2025  \n",
        "**Models:** Logistic Regression, Random Forest, XGBoost  \n",
        "**Validation Method:** 5-Fold Stratified Cross-Validation  \n",
        "**Purpose:** Robust model performance evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('\u2713 Libraries loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load full dataset (train + test combined for CV)\n",
        "X_train = pd.read_csv('X_train.csv')\n",
        "X_test = pd.read_csv('X_test.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
        "\n",
        "# Combine for cross-validation\n",
        "X_full = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "\n",
        "print('\u2713 Data loaded and combined')\n",
        "print(f'Total samples: {len(X_full)}')\n",
        "print(f'Total features: {X_full.shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Define Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models with same parameters as baseline\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        multi_class='ovr',\n",
        "        solver='lbfgs'\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        objective='multi:softprob',\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "}\n",
        "\n",
        "print('\u2713 Models initialized:')\n",
        "for name in models.keys():\n",
        "    print(f'  \u2022 {name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define stratified k-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='macro', zero_division=0),\n",
        "    'recall': make_scorer(recall_score, average='macro', zero_division=0),\n",
        "    'f1': make_scorer(f1_score, average='macro', zero_division=0)\n",
        "}\n",
        "\n",
        "print('\u2713 Cross-validation setup:')\n",
        "print(f'  \u2022 Method: Stratified K-Fold')\n",
        "print(f'  \u2022 Number of folds: 5')\n",
        "print(f'  \u2022 Shuffle: Yes')\n",
        "print(f'  \u2022 Random state: 42')\n",
        "print(f'  \u2022 Metrics: Accuracy, Precision, Recall, F1-Score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Perform Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation for each model\n",
        "cv_results = {}\n",
        "\n",
        "print('='*80)\n",
        "print('RUNNING 5-FOLD CROSS-VALIDATION')\n",
        "print('='*80)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f'\\n{model_name}...')\n",
        "    \n",
        "    # Special handling for Logistic Regression (needs scaling)\n",
        "    if model_name == 'Logistic Regression':\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        model_pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "        scores = cross_validate(model_pipeline, X_full, y_full, \n",
        "                               cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_validate(model, X_full, y_full, \n",
        "                               cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    \n",
        "    cv_results[model_name] = scores\n",
        "    print(f'  \u2713 Completed 5 folds')\n",
        "\n",
        "print('\\n\u2713 Cross-validation completed for all models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame\n",
        "results_data = []\n",
        "\n",
        "for model_name, scores in cv_results.items():\n",
        "    results_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy_Mean': scores['test_accuracy'].mean(),\n",
        "        'Accuracy_Std': scores['test_accuracy'].std(),\n",
        "        'Precision_Mean': scores['test_precision'].mean(),\n",
        "        'Precision_Std': scores['test_precision'].std(),\n",
        "        'Recall_Mean': scores['test_recall'].mean(),\n",
        "        'Recall_Std': scores['test_recall'].std(),\n",
        "        'F1_Mean': scores['test_f1'].mean(),\n",
        "        'F1_Std': scores['test_f1'].std()\n",
        "    })\n",
        "\n",
        "cv_df = pd.DataFrame(results_data)\n",
        "\n",
        "print('='*80)\n",
        "print('CROSS-VALIDATION RESULTS (5-FOLD)')\n",
        "print('='*80)\n",
        "print('\\nFormat: Mean \u00b1 Std')\n",
        "print('-'*80)\n",
        "\n",
        "for idx, row in cv_df.iterrows():\n",
        "    print(f\"\\n{row['Model']}:\")\n",
        "    print(f\"  Accuracy:  {row['Accuracy_Mean']:.4f} \u00b1 {row['Accuracy_Std']:.4f}\")\n",
        "    print(f\"  Precision: {row['Precision_Mean']:.4f} \u00b1 {row['Precision_Std']:.4f}\")\n",
        "    print(f\"  Recall:    {row['Recall_Mean']:.4f} \u00b1 {row['Recall_Std']:.4f}\")\n",
        "    print(f\"  F1-Score:  {row['F1_Mean']:.4f} \u00b1 {row['F1_Std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify best model\n",
        "best_model_idx = cv_df['Accuracy_Mean'].idxmax()\n",
        "best_model_name = cv_df.loc[best_model_idx, 'Model']\n",
        "best_accuracy = cv_df.loc[best_model_idx, 'Accuracy_Mean']\n",
        "best_accuracy_std = cv_df.loc[best_model_idx, 'Accuracy_Std']\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print(f'\ud83c\udfc6 BEST MODEL: {best_model_name}')\n",
        "print(f'   Accuracy: {best_accuracy:.4f} \u00b1 {best_accuracy_std:.4f}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Detailed Fold-by-Fold Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show individual fold scores\n",
        "print('\\n' + '='*80)\n",
        "print('FOLD-BY-FOLD ACCURACY SCORES')\n",
        "print('='*80)\n",
        "\n",
        "fold_data = []\n",
        "\n",
        "for model_name, scores in cv_results.items():\n",
        "    print(f'\\n{model_name}:')\n",
        "    for i, acc in enumerate(scores['test_accuracy'], 1):\n",
        "        print(f'  Fold {i}: {acc:.4f}')\n",
        "        fold_data.append({\n",
        "            'Model': model_name,\n",
        "            'Fold': i,\n",
        "            'Accuracy': acc\n",
        "        })\n",
        "    print(f'  Mean:   {scores[\"test_accuracy\"].mean():.4f}')\n",
        "    print(f'  Std:    {scores[\"test_accuracy\"].std():.4f}')\n",
        "\n",
        "fold_df = pd.DataFrame(fold_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plot of cross-validation scores\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('5-Fold Cross-Validation Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    data_to_plot = [cv_results[model][f'test_{metric}'] for model in models.keys()]\n",
        "    \n",
        "    bp = ax.boxplot(data_to_plot, labels=list(models.keys()), patch_artist=True)\n",
        "    \n",
        "    # Color boxes\n",
        "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "    \n",
        "    ax.set_ylabel(title, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{title} Distribution', fontsize=12, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "    ax.tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cross_validation_boxplots.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: cross_validation_boxplots.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar plot with error bars\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "\n",
        "metrics_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
        "\n",
        "for i, (metric, color) in enumerate(zip(metrics_plot, colors)):\n",
        "    means = cv_df[f'{metric}_Mean'].values\n",
        "    stds = cv_df[f'{metric}_Std'].values\n",
        "    \n",
        "    ax.bar(x + i*width, means, width, label=metric, \n",
        "          yerr=stds, capsize=5, alpha=0.7, color=color)\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Cross-Validation Results with Standard Deviation', \n",
        "            fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(cv_df['Model'].values)\n",
        "ax.legend(loc='lower right', fontsize=10)\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "ax.set_ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cross_validation_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: cross_validation_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fold-by-fold line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for model_name in models.keys():\n",
        "    model_data = fold_df[fold_df['Model'] == model_name]\n",
        "    plt.plot(model_data['Fold'], model_data['Accuracy'], \n",
        "            marker='o', linewidth=2, markersize=8, label=model_name)\n",
        "\n",
        "plt.xlabel('Fold', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "plt.title('Accuracy Across 5 Folds', fontsize=14, fontweight='bold', pad=15)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(range(1, 6))\n",
        "plt.tight_layout()\n",
        "plt.savefig('cross_validation_folds.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: cross_validation_folds.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate confidence intervals (95%)\n",
        "from scipy import stats\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('95% CONFIDENCE INTERVALS')\n",
        "print('='*80)\n",
        "\n",
        "for model_name, scores in cv_results.items():\n",
        "    accuracy_scores = scores['test_accuracy']\n",
        "    mean = accuracy_scores.mean()\n",
        "    std_error = stats.sem(accuracy_scores)\n",
        "    confidence_interval = stats.t.interval(0.95, len(accuracy_scores)-1, \n",
        "                                          loc=mean, scale=std_error)\n",
        "    \n",
        "    print(f'\\n{model_name}:')\n",
        "    print(f'  Mean Accuracy: {mean:.4f}')\n",
        "    print(f'  95% CI: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]')\n",
        "    print(f'  Range: \u00b1{(confidence_interval[1] - mean):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cross-validation results\n",
        "cv_df.to_csv('cross_validation_results.csv', index=False)\n",
        "print('\u2713 Saved: cross_validation_results.csv')\n",
        "\n",
        "# Save fold-by-fold results\n",
        "fold_df.to_csv('cross_validation_folds.csv', index=False)\n",
        "print('\u2713 Saved: cross_validation_folds.csv')\n",
        "\n",
        "# Save detailed results\n",
        "detailed_results = []\n",
        "for model_name, scores in cv_results.items():\n",
        "    for fold in range(5):\n",
        "        detailed_results.append({\n",
        "            'Model': model_name,\n",
        "            'Fold': fold + 1,\n",
        "            'Accuracy': scores['test_accuracy'][fold],\n",
        "            'Precision': scores['test_precision'][fold],\n",
        "            'Recall': scores['test_recall'][fold],\n",
        "            'F1_Score': scores['test_f1'][fold]\n",
        "        })\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_results)\n",
        "detailed_df.to_csv('cross_validation_detailed.csv', index=False)\n",
        "print('\u2713 Saved: cross_validation_detailed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary & Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('CROSS-VALIDATION SUMMARY & RECOMMENDATIONS')\n",
        "print('='*80)\n",
        "\n",
        "print(f'\\n1. Best Performing Model: {best_model_name}')\n",
        "print(f'   Mean Accuracy: {best_accuracy:.4f} \u00b1 {best_accuracy_std:.4f}')\n",
        "\n",
        "# Model stability analysis\n",
        "print('\\n2. Model Stability (based on std deviation):')\n",
        "sorted_by_std = cv_df.sort_values('Accuracy_Std')\n",
        "for idx, row in sorted_by_std.iterrows():\n",
        "    stability = 'High' if row['Accuracy_Std'] < 0.02 else 'Medium' if row['Accuracy_Std'] < 0.05 else 'Low'\n",
        "    print(f\"   {row['Model']}: {stability} (std: {row['Accuracy_Std']:.4f})\")\n",
        "\n",
        "# Performance difference\n",
        "print('\\n3. Performance Differences:')\n",
        "max_acc = cv_df['Accuracy_Mean'].max()\n",
        "min_acc = cv_df['Accuracy_Mean'].min()\n",
        "print(f'   Best to Worst: {(max_acc - min_acc)*100:.2f}% accuracy difference')\n",
        "\n",
        "print('\\n4. Recommendations:')\n",
        "if best_accuracy_std < 0.02:\n",
        "    print(f'   \u2713 {best_model_name} shows excellent stability')\n",
        "    print('   \u2192 Recommended for deployment')\n",
        "elif best_accuracy_std < 0.05:\n",
        "    print(f'   \u2713 {best_model_name} shows good stability')\n",
        "    print('   \u2192 Suitable for deployment with monitoring')\n",
        "else:\n",
        "    print(f'   \u26a0 {best_model_name} shows high variance')\n",
        "    print('   \u2192 Consider hyperparameter tuning or ensemble methods')\n",
        "\n",
        "if max_acc > 0.90:\n",
        "    print('\\n   \u2713 Excellent performance achieved (>90% accuracy)')\n",
        "elif max_acc > 0.80:\n",
        "    print('\\n   \u2713 Good performance (80-90% accuracy)')\n",
        "    print('   \u2192 Consider hyperparameter optimization')\n",
        "else:\n",
        "    print('\\n   \u26a0 Performance needs improvement')\n",
        "    print('   \u2192 Consider advanced feature engineering or ensemble methods')\n",
        "\n",
        "print('\\n5. Next Steps:')\n",
        "print(f'   1. Hyperparameter tuning for {best_model_name}')\n",
        "print('   2. Test on final holdout set')\n",
        "print('   3. Analyze misclassifications')\n",
        "print('   4. Consider ensemble methods if needed')\n",
        "\n",
        "print('\\n' + '='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Cross-Validation Benefits:\n",
        "- **Robust evaluation:** Uses all data for both training and testing\n",
        "- **Reduces variance:** Average across 5 folds provides stable estimate\n",
        "- **Detects overfitting:** High variance indicates model instability\n",
        "- **Fair comparison:** All models evaluated on same folds\n",
        "\n",
        "### Stratified K-Fold:\n",
        "- Maintains class distribution in each fold\n",
        "- Important for imbalanced datasets\n",
        "- Ensures each fold is representative\n",
        "\n",
        "### Files Generated:\n",
        "- cross_validation_results.csv (summary)\n",
        "- cross_validation_folds.csv (fold-by-fold)\n",
        "- cross_validation_detailed.csv (all metrics)\n",
        "- cross_validation_boxplots.png\n",
        "- cross_validation_comparison.png\n",
        "- cross_validation_folds.png\n",
        "\n",
        "### Key Metrics:\n",
        "- **Mean:** Average performance across folds\n",
        "- **Std:** Consistency/stability of performance\n",
        "- **95% CI:** Range where true performance likely lies\n",
        "\n",
        "---\n",
        "**Cross-Validation Complete!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}