{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Importance Analysis & Model Selection\n",
        "## TASK 6: Model Selection - Feature Analysis & Final Model Choice\n",
        "\n",
        "**Date:** December 30, 2025  \n",
        "**Purpose:** Analyze feature importance and select best model  \n",
        "**Based on:** Cross-validation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('\u2713 Libraries loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Load Cross-Validation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CV results\n",
        "cv_results = pd.read_csv('cross_validation_results.csv')\n",
        "\n",
        "print('Cross-Validation Results:')\n",
        "print('='*80)\n",
        "print(cv_results.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model_idx = cv_results['Accuracy_Mean'].idxmax()\n",
        "best_model_name = cv_results.loc[best_model_idx, 'Model']\n",
        "best_accuracy = cv_results.loc[best_model_idx, 'Accuracy_Mean']\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print(f'\ud83c\udfc6 BEST MODEL (by CV): {best_model_name}')\n",
        "print(f'   Mean Accuracy: {best_accuracy:.4f}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Model Selection Criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model selection analysis\n",
        "print('\\n' + '='*80)\n",
        "print('MODEL SELECTION ANALYSIS')\n",
        "print('='*80)\n",
        "\n",
        "# 1. Accuracy ranking\n",
        "print('\\n1. Accuracy Ranking:')\n",
        "sorted_acc = cv_results.sort_values('Accuracy_Mean', ascending=False)\n",
        "for i, (idx, row) in enumerate(sorted_acc.iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Model']}: {row['Accuracy_Mean']:.4f} \u00b1 {row['Accuracy_Std']:.4f}\")\n",
        "\n",
        "# 2. Stability ranking (lowest std)\n",
        "print('\\n2. Stability Ranking (Lower std = More stable):')\n",
        "sorted_std = cv_results.sort_values('Accuracy_Std')\n",
        "for i, (idx, row) in enumerate(sorted_std.iterrows(), 1):\n",
        "    stability = 'Excellent' if row['Accuracy_Std'] < 0.02 else 'Good' if row['Accuracy_Std'] < 0.05 else 'Fair'\n",
        "    print(f\"   {i}. {row['Model']}: {row['Accuracy_Std']:.4f} ({stability})\")\n",
        "\n",
        "# 3. F1-Score ranking\n",
        "print('\\n3. F1-Score Ranking:')\n",
        "sorted_f1 = cv_results.sort_values('F1_Mean', ascending=False)\n",
        "for i, (idx, row) in enumerate(sorted_f1.iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Model']}: {row['F1_Mean']:.4f} \u00b1 {row['F1_Std']:.4f}\")\n",
        "\n",
        "# 4. Overall score (combination)\n",
        "print('\\n4. Combined Score (Accuracy - Std_penalty):')\n",
        "cv_results['Combined_Score'] = cv_results['Accuracy_Mean'] - (cv_results['Accuracy_Std'] * 2)\n",
        "sorted_combined = cv_results.sort_values('Combined_Score', ascending=False)\n",
        "for i, (idx, row) in enumerate(sorted_combined.iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Model']}: {row['Combined_Score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final model selection\n",
        "print('\\n' + '='*80)\n",
        "print('FINAL MODEL SELECTION')\n",
        "print('='*80)\n",
        "\n",
        "selected_model_idx = sorted_combined.index[0]\n",
        "selected_model = cv_results.loc[selected_model_idx]\n",
        "\n",
        "print(f'\\n\u2705 SELECTED MODEL: {selected_model[\"Model\"]}')\n",
        "print('\\nSelection Rationale:')\n",
        "print(f'  \u2022 Accuracy: {selected_model[\"Accuracy_Mean\"]:.4f} \u00b1 {selected_model[\"Accuracy_Std\"]:.4f}')\n",
        "print(f'  \u2022 Precision: {selected_model[\"Precision_Mean\"]:.4f} \u00b1 {selected_model[\"Precision_Std\"]:.4f}')\n",
        "print(f'  \u2022 Recall: {selected_model[\"Recall_Mean\"]:.4f} \u00b1 {selected_model[\"Recall_Std\"]:.4f}')\n",
        "print(f'  \u2022 F1-Score: {selected_model[\"F1_Mean\"]:.4f} \u00b1 {selected_model[\"F1_Std\"]:.4f}')\n",
        "print(f'  \u2022 Combined Score: {selected_model[\"Combined_Score\"]:.4f}')\n",
        "\n",
        "print('\\nWhy this model?')\n",
        "if selected_model['Accuracy_Mean'] == cv_results['Accuracy_Mean'].max():\n",
        "    print('  \u2713 Highest accuracy')\n",
        "if selected_model['Accuracy_Std'] == cv_results['Accuracy_Std'].min():\n",
        "    print('  \u2713 Most stable (lowest variance)')\n",
        "if selected_model['F1_Mean'] == cv_results['F1_Mean'].max():\n",
        "    print('  \u2713 Best F1-Score (balanced metric)')\n",
        "if selected_model['Combined_Score'] == cv_results['Combined_Score'].max():\n",
        "    print('  \u2713 Best combined score (accuracy + stability)')\n",
        "\n",
        "# Save selection\n",
        "final_model_name = selected_model['Model']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Load Data for Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv('X_train.csv')\n",
        "X_test = pd.read_csv('X_test.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
        "\n",
        "# Combine for final training\n",
        "X_full = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "print('\u2713 Data loaded')\n",
        "print(f'Total samples: {len(X_full)}')\n",
        "print(f'Total features: {len(feature_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Feature Importance - All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models on full data to get feature importance\n",
        "print('Training models on full dataset for feature importance analysis...')\n",
        "\n",
        "# Logistic Regression\n",
        "lr_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr'))\n",
        "])\n",
        "lr_model.fit(X_full, y_full)\n",
        "print('\u2713 Logistic Regression trained')\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_full, y_full)\n",
        "print('\u2713 Random Forest trained')\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                              objective='multi:softprob', random_state=42, n_jobs=-1)\n",
        "xgb_model.fit(X_full, y_full)\n",
        "print('\u2713 XGBoost trained')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Logistic Regression - Coefficient Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression coefficients (averaged across classes)\n",
        "lr_classifier = lr_model.named_steps['classifier']\n",
        "lr_coef = np.abs(lr_classifier.coef_).mean(axis=0)\n",
        "\n",
        "lr_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': lr_coef\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('LOGISTIC REGRESSION - TOP 15 FEATURES (by coefficient magnitude)')\n",
        "print('='*80)\n",
        "print(lr_importance.head(15).to_string(index=False))\n",
        "\n",
        "lr_importance.to_csv('feature_importance_logistic_regression_final.csv', index=False)\n",
        "print('\\n\u2713 Saved: feature_importance_logistic_regression_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Random Forest - Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest feature importance\n",
        "rf_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('RANDOM FOREST - TOP 15 FEATURES')\n",
        "print('='*80)\n",
        "print(rf_importance.head(15).to_string(index=False))\n",
        "\n",
        "rf_importance.to_csv('feature_importance_random_forest_final.csv', index=False)\n",
        "print('\\n\u2713 Saved: feature_importance_random_forest_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: XGBoost - Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost feature importance (gain-based)\n",
        "importance_dict = xgb_model.get_booster().get_score(importance_type='gain')\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': [importance_dict.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('XGBOOST - TOP 15 FEATURES (by gain)')\n",
        "print('='*80)\n",
        "print(xgb_importance.head(15).to_string(index=False))\n",
        "\n",
        "xgb_importance.to_csv('feature_importance_xgboost_final.csv', index=False)\n",
        "print('\\n\u2713 Saved: feature_importance_xgboost_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: Compare Feature Importance Across Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize importance to 0-1 scale for comparison\n",
        "lr_norm = (lr_importance['Importance'] - lr_importance['Importance'].min()) / \\\n",
        "          (lr_importance['Importance'].max() - lr_importance['Importance'].min())\n",
        "rf_norm = (rf_importance['Importance'] - rf_importance['Importance'].min()) / \\\n",
        "          (rf_importance['Importance'].max() - rf_importance['Importance'].min())\n",
        "xgb_norm = (xgb_importance['Importance'] - xgb_importance['Importance'].min()) / \\\n",
        "           (xgb_importance['Importance'].max() - xgb_importance['Importance'].min())\n",
        "\n",
        "# Combine\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'LR_Importance': lr_importance.set_index('Feature').loc[feature_names, 'Importance'].values,\n",
        "    'RF_Importance': rf_importance.set_index('Feature').loc[feature_names, 'Importance'].values,\n",
        "    'XGB_Importance': xgb_importance.set_index('Feature').loc[feature_names, 'Importance'].values\n",
        "})\n",
        "\n",
        "# Calculate average importance\n",
        "comparison_df['Average_Importance'] = comparison_df[['LR_Importance', 'RF_Importance', 'XGB_Importance']].mean(axis=1)\n",
        "comparison_df = comparison_df.sort_values('Average_Importance', ascending=False)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('FEATURE IMPORTANCE - CONSENSUS ACROSS ALL MODELS (Top 15)')\n",
        "print('='*80)\n",
        "print(comparison_df.head(15).to_string(index=False))\n",
        "\n",
        "comparison_df.to_csv('feature_importance_comparison.csv', index=False)\n",
        "print('\\n\u2713 Saved: feature_importance_comparison.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 9: Visualize Feature Importance - Selected Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get importance for selected model\n",
        "if final_model_name == 'Logistic Regression':\n",
        "    selected_importance = lr_importance\n",
        "    color = 'Blues'\n",
        "elif final_model_name == 'Random Forest':\n",
        "    selected_importance = rf_importance\n",
        "    color = 'Greens'\n",
        "else:  # XGBoost\n",
        "    selected_importance = xgb_importance\n",
        "    color = 'Oranges'\n",
        "\n",
        "# Plot top 20 features\n",
        "top_features = selected_importance.head(20)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "colors_plot = plt.cm.get_cmap(color)(np.linspace(0.4, 0.8, len(top_features)))\n",
        "plt.barh(range(len(top_features)), top_features['Importance'].values, color=colors_plot)\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'].values)\n",
        "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title(f'Top 20 Feature Importance - {final_model_name} (Selected Model)', \n",
        "         fontsize=14, fontweight='bold', pad=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_selected_model.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: feature_importance_selected_model.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 10: Compare Top Features Across All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison of top 15 features across models\n",
        "top_consensus = comparison_df.head(15)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "x = np.arange(len(top_consensus))\n",
        "width = 0.25\n",
        "\n",
        "# Normalize for fair comparison\n",
        "lr_vals = (top_consensus['LR_Importance'] / top_consensus['LR_Importance'].max()).values\n",
        "rf_vals = (top_consensus['RF_Importance'] / top_consensus['RF_Importance'].max()).values\n",
        "xgb_vals = (top_consensus['XGB_Importance'] / top_consensus['XGB_Importance'].max()).values\n",
        "\n",
        "ax.barh(x - width, lr_vals, width, label='Logistic Regression', color='#3498db', alpha=0.8)\n",
        "ax.barh(x, rf_vals, width, label='Random Forest', color='#2ecc71', alpha=0.8)\n",
        "ax.barh(x + width, xgb_vals, width, label='XGBoost', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(top_consensus['Feature'].values)\n",
        "ax.set_xlabel('Normalized Importance', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Top 15 Features - Comparison Across All Models', \n",
        "            fontsize=14, fontweight='bold', pad=15)\n",
        "ax.legend(loc='lower right', fontsize=10)\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: feature_importance_all_models_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 11: Feature Categories Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorize features\n",
        "def categorize_feature(feature_name):\n",
        "    feature_lower = feature_name.lower()\n",
        "    if 'rate' in feature_lower:\n",
        "        return 'Rate Features'\n",
        "    elif 'ratio' in feature_lower or 'remaining' in feature_lower:\n",
        "        return 'Ratio/Remaining Features'\n",
        "    elif 'interaction' in feature_lower:\n",
        "        return 'Interaction Features'\n",
        "    elif 'category' in feature_lower or 'flag' in feature_lower:\n",
        "        return 'Categorical/Flag Features'\n",
        "    else:\n",
        "        return 'Original Features'\n",
        "\n",
        "comparison_df['Category'] = comparison_df['Feature'].apply(categorize_feature)\n",
        "\n",
        "# Category importance summary\n",
        "category_summary = comparison_df.groupby('Category').agg({\n",
        "    'Average_Importance': ['mean', 'sum', 'count']\n",
        "}).round(4)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('FEATURE CATEGORY ANALYSIS')\n",
        "print('='*80)\n",
        "print(category_summary)\n",
        "\n",
        "print('\\nInterpretation:')\n",
        "print('  \u2022 Mean: Average importance per feature in category')\n",
        "print('  \u2022 Sum: Total contribution of category')\n",
        "print('  \u2022 Count: Number of features in category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 12: Key Insights & Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('KEY INSIGHTS & RECOMMENDATIONS')\n",
        "print('='*80)\n",
        "\n",
        "print(f'\\n1. SELECTED MODEL: {final_model_name}')\n",
        "print(f'   Performance: {selected_model[\"Accuracy_Mean\"]:.4f} \u00b1 {selected_model[\"Accuracy_Std\"]:.4f}')\n",
        "\n",
        "print('\\n2. TOP 5 MOST IMPORTANT FEATURES (Consensus):')\n",
        "for i, (idx, row) in enumerate(comparison_df.head(5).iterrows(), 1):\n",
        "    print(f'   {i}. {row[\"Feature\"]} (Avg importance: {row[\"Average_Importance\"]:.4f})')\n",
        "\n",
        "print('\\n3. FEATURE ENGINEERING IMPACT:')\n",
        "engineered_count = (comparison_df['Category'] != 'Original Features').sum()\n",
        "engineered_top10 = (comparison_df.head(10)['Category'] != 'Original Features').sum()\n",
        "print(f'   \u2022 {engineered_count}/{len(comparison_df)} features are engineered')\n",
        "print(f'   \u2022 {engineered_top10}/10 top features are engineered')\n",
        "if engineered_top10 >= 7:\n",
        "    print('   \u2192 Feature engineering was HIGHLY SUCCESSFUL')\n",
        "elif engineered_top10 >= 4:\n",
        "    print('   \u2192 Feature engineering was SUCCESSFUL')\n",
        "else:\n",
        "    print('   \u2192 Original features remain most important')\n",
        "\n",
        "print('\\n4. RECOMMENDATIONS:')\n",
        "print(f'   \u2713 Deploy {final_model_name} for production')\n",
        "print(f'   \u2713 Monitor top 10 features for data quality')\n",
        "print(f'   \u2713 Consider feature selection to reduce complexity')\n",
        "\n",
        "# Check for low importance features\n",
        "low_importance = comparison_df[comparison_df['Average_Importance'] < 0.001]\n",
        "if len(low_importance) > 0:\n",
        "    print(f'   \u26a0 {len(low_importance)} features have very low importance')\n",
        "    print(f'   \u2192 Consider removing: {low_importance[\"Feature\"].head(5).tolist()}')\n",
        "\n",
        "print('\\n5. NEXT STEPS:')\n",
        "print('   1. Hyperparameter tuning for selected model')\n",
        "print('   2. Test on holdout set')\n",
        "print('   3. Deploy model with monitoring')\n",
        "print('   4. Regular retraining schedule')\n",
        "\n",
        "print('\\n' + '='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 13: Save Final Selection Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create final report\n",
        "final_report = {\n",
        "    'Selected_Model': final_model_name,\n",
        "    'CV_Accuracy_Mean': selected_model['Accuracy_Mean'],\n",
        "    'CV_Accuracy_Std': selected_model['Accuracy_Std'],\n",
        "    'CV_F1_Mean': selected_model['F1_Mean'],\n",
        "    'CV_F1_Std': selected_model['F1_Std'],\n",
        "    'Top_Feature_1': comparison_df.iloc[0]['Feature'],\n",
        "    'Top_Feature_2': comparison_df.iloc[1]['Feature'],\n",
        "    'Top_Feature_3': comparison_df.iloc[2]['Feature'],\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Selection_Date': '2025-12-30'\n",
        "}\n",
        "\n",
        "report_df = pd.DataFrame([final_report])\n",
        "report_df.to_csv('final_model_selection_report.csv', index=False)\n",
        "print('\u2713 Saved: final_model_selection_report.csv')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('ALL FILES SAVED SUCCESSFULLY')\n",
        "print('='*80)\n",
        "print('\\nGenerated files:')\n",
        "print('  \u2022 feature_importance_logistic_regression_final.csv')\n",
        "print('  \u2022 feature_importance_random_forest_final.csv')\n",
        "print('  \u2022 feature_importance_xgboost_final.csv')\n",
        "print('  \u2022 feature_importance_comparison.csv')\n",
        "print('  \u2022 feature_importance_selected_model.png')\n",
        "print('  \u2022 feature_importance_all_models_comparison.png')\n",
        "print('  \u2022 final_model_selection_report.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Model Selection Process:\n",
        "1. **Cross-Validation Results:** Reviewed 5-fold CV performance\n",
        "2. **Selection Criteria:** Accuracy, stability, F1-score, combined score\n",
        "3. **Final Choice:** Selected best performing model\n",
        "\n",
        "### Feature Importance Analysis:\n",
        "- **Logistic Regression:** Coefficient magnitude (absolute values)\n",
        "- **Random Forest:** Mean decrease in impurity\n",
        "- **XGBoost:** Gain-based importance\n",
        "- **Consensus:** Average across all models\n",
        "\n",
        "### Key Findings:\n",
        "- Identified top predictive features\n",
        "- Evaluated feature engineering impact\n",
        "- Found low-value features for potential removal\n",
        "- Provided deployment recommendations\n",
        "\n",
        "### Files Generated:\n",
        "- Feature importance for each model (CSV)\n",
        "- Cross-model comparison (CSV)\n",
        "- Visualizations (PNG)\n",
        "- Final selection report (CSV)\n",
        "\n",
        "---\n",
        "**Analysis Complete! Model selected for deployment.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}