{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison - Pipeline Condition Classification\n",
        "## TASK 5: Baseline Evaluation - Performance Comparison\n",
        "\n",
        "**Date:** December 30, 2025  \n",
        "**Models Compared:** Logistic Regression, Random Forest, XGBoost  \n",
        "**Metrics:** Accuracy, Precision, Recall, F1-Score  \n",
        "**Problem Type:** Multi-class Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('\u2713 Libraries loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Load Predictions from All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load predictions from each model\n",
        "try:\n",
        "    lr_pred = pd.read_csv('logistic_regression_predictions.csv')\n",
        "    print('\u2713 Loaded Logistic Regression predictions')\n",
        "except:\n",
        "    print('\u26a0 Logistic Regression predictions not found')\n",
        "    lr_pred = None\n",
        "\n",
        "try:\n",
        "    rf_pred = pd.read_csv('random_forest_predictions.csv')\n",
        "    print('\u2713 Loaded Random Forest predictions')\n",
        "except:\n",
        "    print('\u26a0 Random Forest predictions not found')\n",
        "    rf_pred = None\n",
        "\n",
        "try:\n",
        "    xgb_pred = pd.read_csv('xgboost_predictions.csv')\n",
        "    print('\u2713 Loaded XGBoost predictions')\n",
        "except:\n",
        "    print('\u26a0 XGBoost predictions not found')\n",
        "    xgb_pred = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load class names\n",
        "import json\n",
        "try:\n",
        "    with open('target_mapping.json', 'r') as f:\n",
        "        target_mapping = json.load(f)\n",
        "    class_names = [k for k, v in sorted(target_mapping.items(), key=lambda x: x[1])]\n",
        "    print(f'\\nClass names: {class_names}')\n",
        "except:\n",
        "    class_names = ['Class_0', 'Class_1', 'Class_2']\n",
        "    print(f'\\nUsing default class names: {class_names}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Calculate Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate all metrics\n",
        "def calculate_metrics(y_true, y_pred, model_name):\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision_Macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'Recall_Macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'F1_Macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'Precision_Weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'Recall_Weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'F1_Weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "print('\u2713 Metric calculation function defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for all models\n",
        "all_metrics = []\n",
        "\n",
        "if lr_pred is not None:\n",
        "    lr_metrics = calculate_metrics(lr_pred['True_Label'], lr_pred['Predicted_Label'], 'Logistic Regression')\n",
        "    all_metrics.append(lr_metrics)\n",
        "\n",
        "if rf_pred is not None:\n",
        "    rf_metrics = calculate_metrics(rf_pred['True_Label'], rf_pred['Predicted_Label'], 'Random Forest')\n",
        "    all_metrics.append(rf_metrics)\n",
        "\n",
        "if xgb_pred is not None:\n",
        "    xgb_metrics = calculate_metrics(xgb_pred['True_Label'], xgb_pred['Predicted_Label'], 'XGBoost')\n",
        "    all_metrics.append(xgb_metrics)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "print('\u2713 Metrics calculated for all models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison table\n",
        "print('='*90)\n",
        "print('MODEL PERFORMANCE COMPARISON')\n",
        "print('='*90)\n",
        "print('\\nOverall Metrics:')\n",
        "print('-'*90)\n",
        "\n",
        "display_df = metrics_df[['Model', 'Accuracy', 'Precision_Macro', 'Recall_Macro', 'F1_Macro']].copy()\n",
        "display_df.columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "# Round to 4 decimals\n",
        "for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
        "    display_df[col] = display_df[col].round(4)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Highlight best model\n",
        "best_model_idx = display_df['Accuracy'].idxmax()\n",
        "best_model = display_df.loc[best_model_idx, 'Model']\n",
        "best_accuracy = display_df.loc[best_model_idx, 'Accuracy']\n",
        "\n",
        "print('\\n' + '='*90)\n",
        "print(f'\ud83c\udfc6 BEST MODEL: {best_model} (Accuracy: {best_accuracy:.4f})')\n",
        "print('='*90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Per-Class Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-class metrics for each model\n",
        "print('\\n' + '='*90)\n",
        "print('PER-CLASS PERFORMANCE COMPARISON')\n",
        "print('='*90)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'\\n{class_name}:')\n",
        "    print('-'*60)\n",
        "    \n",
        "    class_metrics = []\n",
        "    \n",
        "    if lr_pred is not None:\n",
        "        lr_prec = precision_score(lr_pred['True_Label'], lr_pred['Predicted_Label'], \n",
        "                                 labels=[i], average=None, zero_division=0)[0]\n",
        "        lr_rec = recall_score(lr_pred['True_Label'], lr_pred['Predicted_Label'], \n",
        "                             labels=[i], average=None, zero_division=0)[0]\n",
        "        lr_f1 = f1_score(lr_pred['True_Label'], lr_pred['Predicted_Label'], \n",
        "                        labels=[i], average=None, zero_division=0)[0]\n",
        "        class_metrics.append(['Logistic Regression', lr_prec, lr_rec, lr_f1])\n",
        "    \n",
        "    if rf_pred is not None:\n",
        "        rf_prec = precision_score(rf_pred['True_Label'], rf_pred['Predicted_Label'], \n",
        "                                 labels=[i], average=None, zero_division=0)[0]\n",
        "        rf_rec = recall_score(rf_pred['True_Label'], rf_pred['Predicted_Label'], \n",
        "                             labels=[i], average=None, zero_division=0)[0]\n",
        "        rf_f1 = f1_score(rf_pred['True_Label'], rf_pred['Predicted_Label'], \n",
        "                        labels=[i], average=None, zero_division=0)[0]\n",
        "        class_metrics.append(['Random Forest', rf_prec, rf_rec, rf_f1])\n",
        "    \n",
        "    if xgb_pred is not None:\n",
        "        xgb_prec = precision_score(xgb_pred['True_Label'], xgb_pred['Predicted_Label'], \n",
        "                                  labels=[i], average=None, zero_division=0)[0]\n",
        "        xgb_rec = recall_score(xgb_pred['True_Label'], xgb_pred['Predicted_Label'], \n",
        "                              labels=[i], average=None, zero_division=0)[0]\n",
        "        xgb_f1 = f1_score(xgb_pred['True_Label'], xgb_pred['Predicted_Label'], \n",
        "                         labels=[i], average=None, zero_division=0)[0]\n",
        "        class_metrics.append(['XGBoost', xgb_prec, xgb_rec, xgb_f1])\n",
        "    \n",
        "    class_df = pd.DataFrame(class_metrics, columns=['Model', 'Precision', 'Recall', 'F1-Score'])\n",
        "    for col in ['Precision', 'Recall', 'F1-Score']:\n",
        "        class_df[col] = class_df[col].round(4)\n",
        "    print(class_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Visualize Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    col_name = metric if metric == 'Accuracy' else f'{metric.replace(\"-\", \"_\")}_Macro'\n",
        "    values = display_df[metric].values\n",
        "    models = display_df['Model'].values\n",
        "    \n",
        "    bars = ax.bar(models, values, color=colors[:len(models)], alpha=0.7)\n",
        "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylim(0, 1.1)\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "               f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "    \n",
        "    # Rotate x-labels if needed\n",
        "    ax.tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: model_comparison_metrics.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Radar chart for overall comparison\n",
        "from math import pi\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "metrics_radar = ['Accuracy', 'Precision_Macro', 'Recall_Macro', 'F1_Macro']\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "angles = [n / float(len(labels)) * 2 * pi for n in range(len(labels))]\n",
        "angles += angles[:1]\n",
        "\n",
        "ax.set_theta_offset(pi / 2)\n",
        "ax.set_theta_direction(-1)\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels, fontsize=11, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "for idx, row in metrics_df.iterrows():\n",
        "    values = [row[m] for m in metrics_radar]\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])\n",
        "    ax.fill(angles, values, alpha=0.15)\n",
        "\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
        "ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
        "print('\u2713 Saved: model_comparison_radar.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Save Comparison Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save complete metrics table\n",
        "metrics_df.to_csv('model_comparison_metrics.csv', index=False)\n",
        "print('\u2713 Saved: model_comparison_metrics.csv')\n",
        "\n",
        "# Save summary table\n",
        "display_df.to_csv('model_comparison_summary.csv', index=False)\n",
        "print('\u2713 Saved: model_comparison_summary.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank models by different metrics\n",
        "print('\\n' + '='*90)\n",
        "print('MODEL RANKING BY METRIC')\n",
        "print('='*90)\n",
        "\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
        "    ranked = display_df.sort_values(metric, ascending=False)\n",
        "    print(f'\\n{metric}:')\n",
        "    for i, (idx, row) in enumerate(ranked.iterrows(), 1):\n",
        "        print(f'  {i}. {row[\"Model\"]}: {row[metric]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Key Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*90)\n",
        "print('KEY INSIGHTS & RECOMMENDATIONS')\n",
        "print('='*90)\n",
        "\n",
        "# Find best performing model\n",
        "best_acc = display_df.loc[display_df['Accuracy'].idxmax()]\n",
        "best_f1 = display_df.loc[display_df['F1-Score'].idxmax()]\n",
        "\n",
        "print(f'\\n1. Best Overall Accuracy: {best_acc[\"Model\"]} ({best_acc[\"Accuracy\"]:.4f})')\n",
        "print(f'2. Best F1-Score: {best_f1[\"Model\"]} ({best_f1[\"F1-Score\"]:.4f})')\n",
        "\n",
        "# Calculate improvement\n",
        "if len(display_df) > 1:\n",
        "    sorted_acc = display_df.sort_values('Accuracy', ascending=False)\n",
        "    improvement = (sorted_acc.iloc[0]['Accuracy'] - sorted_acc.iloc[-1]['Accuracy']) * 100\n",
        "    print(f'\\n3. Performance Improvement: {improvement:.2f}% from worst to best model')\n",
        "\n",
        "print('\\n4. Recommendations:')\n",
        "if best_acc['Accuracy'] > 0.90:\n",
        "    print('   \u2713 Excellent performance achieved (>90% accuracy)')\n",
        "    print(f'   \u2192 Deploy {best_acc[\"Model\"]} for production')\n",
        "elif best_acc['Accuracy'] > 0.80:\n",
        "    print('   \u2713 Good performance (80-90% accuracy)')\n",
        "    print(f'   \u2192 Consider hyperparameter tuning for {best_acc[\"Model\"]}')\n",
        "else:\n",
        "    print('   \u26a0 Performance needs improvement (<80% accuracy)')\n",
        "    print('   \u2192 Try feature engineering, hyperparameter tuning, or ensemble methods')\n",
        "\n",
        "print('\\n5. Model Selection:')\n",
        "print(f'   \u2022 For deployment: {best_acc[\"Model\"]} (highest accuracy)')\n",
        "print(f'   \u2022 For interpretability: Logistic Regression (if included)')\n",
        "print(f'   \u2022 For balance: {best_f1[\"Model\"]} (best F1-score)')\n",
        "\n",
        "print('\\n' + '='*90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Classification Metrics Explained:\n",
        "\n",
        "- **Accuracy:** Overall correctness (correct predictions / total predictions)\n",
        "- **Precision:** Of predicted positives, how many are actually positive (TP / (TP + FP))\n",
        "- **Recall:** Of actual positives, how many were correctly predicted (TP / (TP + FN))\n",
        "- **F1-Score:** Harmonic mean of precision and recall (2 * P * R / (P + R))\n",
        "\n",
        "### Macro vs Weighted Averaging:\n",
        "\n",
        "- **Macro:** Simple average across all classes (treats each class equally)\n",
        "- **Weighted:** Average weighted by class support (accounts for class imbalance)\n",
        "\n",
        "### Files Generated:\n",
        "- model_comparison_metrics.csv (detailed metrics)\n",
        "- model_comparison_summary.csv (summary table)\n",
        "- model_comparison_metrics.png (bar charts)\n",
        "- model_comparison_radar.png (radar chart)\n",
        "\n",
        "### Next Steps:\n",
        "1. Select best performing model\n",
        "2. Perform hyperparameter tuning\n",
        "3. Analyze misclassifications\n",
        "4. Consider ensemble methods\n",
        "\n",
        "---\n",
        "**Model Comparison Complete!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}